{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:13:48.293673Z",
     "start_time": "2025-12-03T14:01:31.833676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Model Paths ---\n",
    "MODEL_DETECT_PATH = 'data/model/detect_yolo_small_v3.pt'\n",
    "MODEL_CLASSIFY_PATH = 'data/model/classify_yolo_small_v3.pt'\n",
    "\n",
    "# --- Video Path ---\n",
    "# *** CHANGE THIS to the path of your video file ***\n",
    "VIDEO_PATH = 'F:/GREEN.mp4'\n",
    "\n",
    "# --- Output Directory Paths ---\n",
    "# The output directory will be created next to the video file:\n",
    "# e.g., 'D:/Recordings/New_Recordings/Round2/classes/'\n",
    "VIDEO_DIR = os.path.dirname(VIDEO_PATH)\n",
    "OUTPUT_BASE_DIR = os.path.join(VIDEO_DIR, 'classes')\n",
    "\n",
    "# --- Processing Parameters ---\n",
    "# Confidence threshold for the DETECT model\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "# Process every Nth frame (e.g., 1 processes every frame, 10 processes every tenth frame)\n",
    "FRAME_SKIP = 100\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Setup: Loading YOLO models...\")\n",
    "# Load the YOLO Detection Model\n",
    "try:\n",
    "    model_detect = YOLO(MODEL_DETECT_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading detection model: {e}\")\n",
    "    print(\"Please check if MODEL_DETECT_PATH is correct.\")\n",
    "    exit()\n",
    "\n",
    "# Load the YOLO Classification Model\n",
    "try:\n",
    "    model_classify = YOLO(MODEL_CLASSIFY_PATH, task='classify')\n",
    "    CLASSIFY_NAMES = model_classify.names\n",
    "    print(f\"Classification Model Classes: {CLASSIFY_NAMES}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classification model: {e}\")\n",
    "    print(\"Please check if MODEL_CLASSIFY_PATH is correct.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the base output directory exists\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "print(f\"Output directory created at: {OUTPUT_BASE_DIR}\")\n",
    "print(\"Setup Complete. Starting video processing...\")\n",
    "\n",
    "# Dictionary to hold the next sequential ID for each class, shared across all frames\n",
    "class_counters = {}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def process_frame(img, frame_number, class_counters):\n",
    "    \"\"\"\n",
    "    Performs detection, cropping, classification, and saves the cropped image\n",
    "    for a single frame.\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        return class_counters # Skip if frame load failed\n",
    "\n",
    "    # 1. Run Detection\n",
    "    # Run the detection model on the image\n",
    "    results_detect = model_detect(img, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
    "\n",
    "    # Process results from the first (and only) image in the batch\n",
    "    if not results_detect or not results_detect[0].boxes:\n",
    "        return class_counters\n",
    "\n",
    "    detections = results_detect[0].boxes\n",
    "\n",
    "    # 2. Iterate through Detections\n",
    "    for i, box in enumerate(detections):\n",
    "        # Get bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "        # 3. Crop the Detected Object\n",
    "        # Ensure coordinates are valid and within image boundaries\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(img.shape[1], x2)\n",
    "        y2 = min(img.shape[0], y2)\n",
    "\n",
    "        # Perform the crop\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "\n",
    "        # Skip if the crop is empty (e.g., invalid coordinates or too small)\n",
    "        if crop_img.size == 0 or crop_img.shape[0] < 5 or crop_img.shape[1] < 5:\n",
    "            continue\n",
    "\n",
    "        # 4. Classify the Cropped Object\n",
    "        # Run the classification model on the cropped image\n",
    "        results_classify = model_classify(crop_img, verbose=False)\n",
    "\n",
    "        # Get the top class ID and name\n",
    "        top_class_id = results_classify[0].probs.top1\n",
    "        class_name = CLASSIFY_NAMES[top_class_id]\n",
    "\n",
    "        # 5. Save to its Class Type Folder with Sequential Naming\n",
    "        # Define the class-specific output directory: {OUTPUT_BASE_DIR}/{class_name}\n",
    "        class_output_dir = os.path.join(OUTPUT_BASE_DIR, class_name)\n",
    "        os.makedirs(class_output_dir, exist_ok=True)\n",
    "\n",
    "        # Get the current sequence counter and increment\n",
    "        if class_name not in class_counters:\n",
    "            class_counters[class_name] = 1\n",
    "\n",
    "        current_id = class_counters[class_name]\n",
    "\n",
    "        # Format the filename: %04d.jpg (e.g., 0001.jpg)\n",
    "        crop_filename = f\"{current_id:04d}.jpg\"\n",
    "        output_path = os.path.join(class_output_dir, crop_filename)\n",
    "\n",
    "        # Save the cropped image\n",
    "        cv2.imwrite(output_path, crop_img)\n",
    "\n",
    "        # Increment the counter for the next crop of this class\n",
    "        class_counters[class_name] += 1\n",
    "\n",
    "    return class_counters\n",
    "\n",
    "def process_video(video_path, class_counters):\n",
    "    \"\"\"\n",
    "    Reads the video, extracts frames, and calls process_frame on each.\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file at {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Video loaded: {os.path.basename(video_path)} | Frames: {frame_count} | FPS: {fps:.2f}\")\n",
    "\n",
    "    frame_number = 0\n",
    "    # Use tqdm to show progress through the video frames\n",
    "    with tqdm(total=frame_count, desc=\"Processing Video Frames\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break # End of video\n",
    "\n",
    "            frame_number += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Skip frames based on FRAME_SKIP configuration\n",
    "            if frame_number % FRAME_SKIP != 0:\n",
    "                continue\n",
    "\n",
    "            # Process the frame and update the counters\n",
    "            process_frame(frame, frame_number, class_counters)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    if class_counters:\n",
    "        total_crops = sum(class_counters.values()) - len(class_counters)\n",
    "        print(f\"Finished processing video. Total {total_crops} crops saved.\")\n",
    "        for class_name, count in class_counters.items():\n",
    "             print(f\"- '{class_name}': {count - 1} crops saved.\")\n",
    "    else:\n",
    "        print(\"Finished processing video. No objects were detected and saved.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "if not os.path.exists(VIDEO_PATH):\n",
    "    print(f\"\\nFATAL ERROR: The configured VIDEO_PATH does not exist: {VIDEO_PATH}\")\n",
    "    print(\"Please update the 'VIDEO_PATH' variable in section 1 of the script.\")\n",
    "else:\n",
    "    process_video(VIDEO_PATH, class_counters)\n",
    "    print(f\"\\nFinal crops output folder: {OUTPUT_BASE_DIR}\")"
   ],
   "id": "df54c9ce43db1ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Loading YOLO models...\n",
      "Classification Model Classes: {0: 'Blue_Yellow', 1: 'Bran', 2: 'Brown_Orange_Overlay', 3: 'Brown_Orange_Small', 4: 'Green_Yellow', 5: 'Red_Yellow', 6: 'Wheatberry'}\n",
      "Output directory created at: F:/classes\n",
      "Setup Complete. Starting video processing...\n",
      "Video loaded: GREEN.mp4 | Frames: 71988 | FPS: 19.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing Video Frames:   0%|          | 0/71988 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3cbe58a48d34de6b5819c94f01ab7cb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 189\u001B[39m\n\u001B[32m    187\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPlease update the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mVIDEO_PATH\u001B[39m\u001B[33m'\u001B[39m\u001B[33m variable in section 1 of the script.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m     \u001B[43mprocess_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mVIDEO_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_counters\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    190\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mFinal crops output folder: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mOUTPUT_BASE_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 153\u001B[39m, in \u001B[36mprocess_video\u001B[39m\u001B[34m(video_path, class_counters)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total=frame_count, desc=\u001B[33m\"\u001B[39m\u001B[33mProcessing Video Frames\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m cap.isOpened():\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m         ret, frame = \u001B[43mcap\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    155\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[32m    156\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m \u001B[38;5;66;03m# End of video\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
