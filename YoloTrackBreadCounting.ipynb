{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:18:21.492162Z",
     "start_time": "2025-12-01T18:17:23.330327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# RECOMMENDATION: Use .engine (TensorRT) or .onnx files for 2x-5x speedup\n",
    "MODEL_PATH = 'data/model/detect_yolo_small_v3.pt'  # Change to 'best5.engine' after exporting\n",
    "MODEL_CLASSIFY_PATH = 'data/model/classify_yolo_nano_v2.pt.pt'  # Change to 'best_classify.engine' after exporting\n",
    "VIDEO_PATH = \"output.mp4\"\n",
    "TRACKER_CONFIG = 'custom_bytetrack.yaml'\n",
    "\n",
    "N_FRAMES_OPEN_CONFIRM = 3\n",
    "M_FRAMES_CLOSED_CONFIRM = 4\n",
    "\n",
    "# Reduced visualization resolution (width) for faster display\n",
    "DISPLAY_WIDTH = 1280\n",
    "# --- END OF CONFIGURATION ---\n",
    "\n",
    "# --- Shared Resources for Threading ---\n",
    "classification_queue = queue.Queue()\n",
    "class_counts = defaultdict(int)\n",
    "counts_lock = threading.Lock() # Prevents race conditions when reading/writing counts\n",
    "\n",
    "def classifier_worker():\n",
    "    \"\"\"\n",
    "    Worker thread that waits for ROIs (images of bags) and classifies them.\n",
    "    This runs in the background so the video doesn't stutter.\n",
    "    \"\"\"\n",
    "    print(\"Loading classification model in worker thread...\")\n",
    "    # Load model inside the thread or pass it in.\n",
    "    # NOTE: For TensorRT/Exported models, loading here is safer for thread isolation.\n",
    "    model_classify = YOLO(MODEL_CLASSIFY_PATH)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for an item, but allow checking for exit every second\n",
    "            item = classification_queue.get(timeout=1)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "        if item is None: # Sentinel to kill thread\n",
    "            break\n",
    "\n",
    "        track_id, roi_img = item\n",
    "\n",
    "        # Run inference (verbose=False is slightly faster)\n",
    "        results = model_classify(roi_img, verbose=False)\n",
    "\n",
    "        detected_label = \"Unknown\"\n",
    "        # Logic to extract label\n",
    "        if results[0].probs is not None:\n",
    "            detected_label = results[0].names[results[0].probs.top1]\n",
    "        elif len(results[0].boxes) > 0:\n",
    "            top_cls = int(results[0].boxes.cls[0])\n",
    "            detected_label = model_classify.names[top_cls]\n",
    "\n",
    "        with counts_lock:\n",
    "            class_counts[detected_label] += 1\n",
    "\n",
    "        # print(f\" [Async] Track {track_id} -> {detected_label}\")\n",
    "        classification_queue.task_done()\n",
    "\n",
    "# Start the classification thread\n",
    "t = threading.Thread(target=classifier_worker, daemon=True)\n",
    "t.start()\n",
    "\n",
    "# --- Main Tracker Setup ---\n",
    "model = YOLO(MODEL_PATH)\n",
    "class_names = model.names\n",
    "\n",
    "try:\n",
    "    OPEN_CLASS_NAME = 'bread-bag-opened'\n",
    "    CLOSED_CLASS_NAME = 'bread-bag-closed'\n",
    "    names_to_ids = {v: k for k, v in class_names.items()}\n",
    "    open_class_id = names_to_ids[OPEN_CLASS_NAME]\n",
    "    closed_class_id = names_to_ids[CLOSED_CLASS_NAME]\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Class {e} not found.\")\n",
    "    exit()\n",
    "\n",
    "track_states = {}\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# Optimization: Pre-calculate resize factor for display to avoid resizing full arrays constantly if not needed\n",
    "# Or simply resize the final image.\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Optimization: Run tracking.\n",
    "    # 'persist=True' is required for tracking.\n",
    "    # 'verbose=False' suppresses printing to console (saves I/O time)\n",
    "    results = model.track(frame, persist=True, tracker=TRACKER_CONFIG, verbose=False)\n",
    "\n",
    "    # Extract data locally to avoid repeated access\n",
    "    boxes = results[0].boxes\n",
    "    current_detections = {}\n",
    "\n",
    "    # Optimization: Manual Drawing is faster than results[0].plot()\n",
    "    # We will draw on 'frame' directly.\n",
    "    if boxes.id is not None:\n",
    "        # Move tensors to CPU once and convert to numpy/list\n",
    "        track_ids = boxes.id.int().cpu().tolist()\n",
    "        cls_ids = boxes.cls.int().cpu().tolist()\n",
    "        xyxys = boxes.xyxy.cpu().tolist()\n",
    "\n",
    "        for track_id, cls_id, box in zip(track_ids, cls_ids, xyxys):\n",
    "            current_detections[track_id] = cls_id\n",
    "\n",
    "            # Draw Bounding Box (Green for Closed, Red for Open, White for others)\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            color = (200, 200, 200)\n",
    "            if cls_id == open_class_id: color = (0, 0, 255)     # Red\n",
    "            elif cls_id == closed_class_id: color = (0, 255, 0) # Green\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            # Draw Label (Simplified)\n",
    "            label = f\"{track_id}: {class_names[cls_id]}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "            # --- State Machine Logic ---\n",
    "            # Retrieve or create state\n",
    "            if track_id not in track_states:\n",
    "                track_states[track_id] = {'state': 'detecting_open', 'open_count': 0, 'closed_count': 0}\n",
    "\n",
    "            state_info = track_states[track_id]\n",
    "\n",
    "            # Logic\n",
    "            if state_info['state'] == 'detecting_open':\n",
    "                if cls_id == open_class_id:\n",
    "                    state_info['open_count'] += 1\n",
    "                else:\n",
    "                    state_info['open_count'] = 0\n",
    "\n",
    "                if state_info['open_count'] >= N_FRAMES_OPEN_CONFIRM:\n",
    "                    state_info['state'] = 'detecting_closed'\n",
    "                    state_info['open_count'] = 0\n",
    "\n",
    "            elif state_info['state'] == 'detecting_closed':\n",
    "                if cls_id == closed_class_id:\n",
    "                    state_info['closed_count'] += 1\n",
    "                else:\n",
    "                    state_info['closed_count'] = 0\n",
    "\n",
    "                if state_info['closed_count'] >= M_FRAMES_CLOSED_CONFIRM:\n",
    "                    state_info['state'] = 'counted'\n",
    "\n",
    "                    # --- ASYNC CLASSIFICATION TRIGGER ---\n",
    "                    # Validate coordinates\n",
    "                    h, w, _ = frame.shape\n",
    "                    cx1, cy1 = max(0, x1), max(0, y1)\n",
    "                    cx2, cy2 = min(w, x2), min(h, y2)\n",
    "\n",
    "                    if cx2 > cx1 and cy2 > cy1:\n",
    "                        # Copy ROI to avoid memory issues when frame changes\n",
    "                        roi = frame[cy1:cy2, cx1:cx2].copy()\n",
    "                        # Push to queue\n",
    "                        classification_queue.put((track_id, roi))\n",
    "\n",
    "    # Clean up old tracks from state dictionary\n",
    "    # (Optional: Keeps dictionary small)\n",
    "    for tid in list(track_states.keys()):\n",
    "        if tid not in current_detections:\n",
    "            # You might want to keep them briefly or delete immediately\n",
    "            # Here we just reset counts if lost\n",
    "            track_states[tid]['open_count'] = 0\n",
    "            track_states[tid]['closed_count'] = 0\n",
    "\n",
    "    # --- Display Stats ---\n",
    "    # Read counts safely\n",
    "    with counts_lock:\n",
    "        display_counts = list(class_counts.items())\n",
    "\n",
    "    y_offset = 60\n",
    "    cv2.putText(frame, \"FPS: calculate if needed\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 1.6, (0, 255, 255), 3)\n",
    "\n",
    "    sorted_counts = sorted(display_counts)\n",
    "    print(f\"sorted_counts: {sorted_counts}\")\n",
    "    for cls_name, count in sorted_counts:\n",
    "        y_offset += 70\n",
    "        text = f\"{cls_name}: {count}\"\n",
    "        cv2.putText(frame, text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
    "\n",
    "    # Resize only for display (keeps processing on original resolution)\n",
    "    if DISPLAY_WIDTH:\n",
    "        h, w = frame.shape[:2]\n",
    "        scale = DISPLAY_WIDTH / w\n",
    "        frame_disp = cv2.resize(frame, (int(w*scale), int(h*scale)))\n",
    "    else:\n",
    "        frame_disp = frame\n",
    "\n",
    "    cv2.imshow(\"Optimized Tracker\", frame_disp)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "4ca979936b913618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classification model in worker thread...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12 (classifier_worker):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 1052, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 989, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Khaled\\AppData\\Local\\Temp\\ipykernel_21952\\2566700008.py\", line 35, in classifier_worker\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py\", line 81, in __init__\n",
      "    super().__init__(model=model, task=task, verbose=verbose)\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py\", line 149, in __init__\n",
      "    self._load(model, task=task)\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py\", line 288, in _load\n",
      "    self.model, self.ckpt = load_checkpoint(weights)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 1461, in load_checkpoint\n",
      "    ckpt, weight = torch_safe_load(weight)  # load ckpt\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py\", line 1409, in torch_safe_load\n",
      "    ckpt = torch_load(file, map_location=\"cpu\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py\", line 116, in torch_load\n",
      "    return torch.load(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1479, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 759, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Khaled\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 740, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data\\\\model\\\\classify_yolo_nano_v2.pt.pt'\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T12:29:31.257821Z",
     "start_time": "2025-12-01T12:29:31.138843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('data/model/classify_yolo_nano_v2.pt')\n",
    "\n",
    "# 2. Get the class names dictionary\n",
    "class_dict = model.names\n",
    "print(f\"Detected Classes: {class_dict}\")"
   ],
   "id": "4d5096c4bffdd031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Classes: {0: 'Blue_Yellow', 1: 'Bran', 2: 'Brown_Orange_Overlay', 3: 'Brown_Orange_Small', 4: 'Green_Yellow', 5: 'Red_Yellow', 6: 'Wheatberry'}\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
